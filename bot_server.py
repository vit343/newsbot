
import asyncio
import aiohttp
import feedparser
import logging
import os
from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab, Doc
import re
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Set
import json
from dataclasses import dataclass
import hashlib
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Natasha
segmenter = Segmenter()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
morph_vocab = MorphVocab()

# –°–∞–º–∞—Ä—Å–∫–∞—è timezone (GMT+4)
SAMARA_TZ = timezone(timedelta(hours=4))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ HTTP –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ—Å—Ç—ã—Ö –∞–Ω—Ç–∏–±–æ—Ç-–∑–∞—â–∏—Ç
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
    ),
    # –í–∫–ª—é—á–∞–µ–º RSS/XML mime-—Ç–∏–ø—ã, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å 406 (Not Acceptable)
    "Accept": "application/rss+xml, application/xml;q=0.9, text/xml;q=0.9, text/html;q=0.8, */*;q=0.7",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Cache-Control": "no-cache",
}
MAX_FETCH_RETRIES = 3
BASE_BACKOFF_SEC = 1.0

# –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∑–µ—Ä–∫–∞–ª–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –Ω–µ—Ç alt_urls)
MIRROR_FALLBACKS: dict[str, list[str]] = {
    # –∫–ª—é—á–∏ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ; –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ —Ä—É—Å—Å–∫–∏–µ, —Ç–∞–∫ –∏ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π
    "rbc": [
        "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
        "https://news.google.com/rss/search?q=site:rbc.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "—Ä–±–∫": [
        "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
        "https://news.google.com/rss/search?q=site:rbc.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "moex": [
        "https://www.moex.com/ru/news/rss",
        "https://www.moex.com/en/news/rss",
        "https://news.google.com/rss/search?q=site:moex.com&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "cbr": [
        "https://www.cbr.ru/rss/press/",
        "https://www.cbr.ru/eng/rss/press/",
        "https://news.google.com/rss/search?q=site:cbr.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "—Ü–± —Ä—Ñ": [
        "https://www.cbr.ru/rss/press/",
        "https://www.cbr.ru/eng/rss/press/",
        "https://news.google.com/rss/search?q=site:cbr.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "kommersant": [
        "https://www.kommersant.ru/RSS/news.xml",
        "https://news.google.com/rss/search?q=site:kommersant.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "–∫–æ–º–º–µ—Ä—Å–∞–Ω—Ç": [
        "https://www.kommersant.ru/RSS/news.xml",
        "https://news.google.com/rss/search?q=site:kommersant.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "finmarket": [
        "https://www.finmarket.ru/rss/news.xml",
        "https://news.google.com/rss/search?q=site:finmarket.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
    "—Ñ–∏–Ω–º–∞—Ä–∫–µ—Ç": [
        "https://www.finmarket.ru/rss/news.xml",
        "https://news.google.com/rss/search?q=site:finmarket.ru&hl=ru&gl=RU&ceid=RU:ru",
    ],
}

# –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤
SYNONYMS = {
    '–±–∞–Ω–∫': ['–±–∞–Ω–∫', '–±–∞–Ω–∫–æ–≤—Å–∫–∏–π', '–∫—Ä–µ–¥–∏—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ'],
    '–∏–Ω—Ñ–ª—è—Ü–∏—è': ['–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Ä–æ—Å—Ç —Ü–µ–Ω', '–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ'],
    '–∫—Ä–∏–∑–∏—Å': ['–∫—Ä–∏–∑–∏—Å', '—Å–ø–∞–¥', '—Ä–µ—Ü–µ—Å—Å–∏—è'],
}

def normalize_text_natasha(text: str) -> set[str]:
    doc = Doc(text.lower())
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)
    lemmas = set()
    for token in doc.tokens:
        token.lemmatize(morph_vocab)
        if re.match(r'\w+', token.text):
            lemmas.add(token.lemma)
    return lemmas

def match_with_synonyms(title: str, keywords: list[str]) -> bool:
    lemmas = normalize_text_natasha(title)
    for keyword in keywords:
        doc = Doc(keyword.lower())
        doc.segment(segmenter)
        doc.tag_morph(morph_tagger)
        if doc.tokens:
            doc.tokens[0].lemmatize(morph_vocab)
            lemma = doc.tokens[0].lemma
        else:
            lemma = keyword.lower()
        all_forms = {lemma} | set(SYNONYMS.get(lemma, []))
        if lemmas & all_forms:
            return True
    return False

@dataclass
class NewsItem:
    title: str
    url: str
    source: str
    priority: int
    category: str
    timestamp: datetime
    hash: str
    via_mirror: bool = False  # –ø–æ–ª—É—á–µ–Ω–∞ —á–µ—Ä–µ–∑ –∑–µ—Ä–∫–∞–ª–æ (alt_urls), –Ω–∞–ø—Ä–∏–º–µ—Ä Google News

class RussianMarketNewsBot:
    def __init__(self, bot_token: str, chat_id: str):
        self.bot_token = bot_token
        self.chat_id = chat_id
        self.seen_news: Set[str] = set()
        self.is_running = False
        self.config_file = "rss_sources.json"
        self.filter_file = "news_filters.json"
        
        self.rss_sources = self.load_sources()
        self.filters = self.load_filters()
        
        self.critical_keywords = [
            '–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞', '—Å–∞–Ω–∫—Ü–∏–∏', '–≥–∞–∑–ø—Ä–æ–º', '—Å–±–µ—Ä–±–∞–Ω–∫', '–ª—É–∫–æ–π–ª', '—Ä–æ—Å–Ω–µ—Ñ—Ç',
            '—Ü–± —Ä—Ñ', '–±–∞–Ω–∫ —Ä–æ—Å—Å–∏–∏', '–Ω–∞–±–∏—É–ª–ª–∏–Ω–∞', '–º–∏—à—É—Å—Ç–∏–Ω', '—Å–∏–ª—É–∞–Ω–æ–≤',
            '–∫—É—Ä—Å —Ä—É–±–ª—è', '–Ω–µ—Ñ—Ç—å', '–∑–æ–ª–æ—Ç–æ', '–∏–Ω—Ñ–ª—è—Ü–∏—è'
        ]
        
        self.tracked_companies = [
            '–≥–∞–∑–ø—Ä–æ–º', '—Å–±–µ—Ä–±–∞–Ω–∫', '–ª—É–∫–æ–π–ª', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '–Ω–æ—Ä–Ω–∏–∫–µ–ª—å',
            '—è–Ω–¥–µ–∫—Å', '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '–≤—ã–º–ø–µ–ª–∫–æ–º', '–º—Ç—Å', '–º–µ–≥–∞—Ñ–æ–Ω',
            '—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å', '–Ω–ª–º–∫', '–Ω–æ–≤–∞—Ç—ç–∫', '–º–∞–≥–Ω–∏—Ç', '—Ö5',
            '—Å—É—Ä–≥—É—Ç–Ω–µ—Ñ—Ç–µ–≥–∞–∑', '—Ç–∞—Ç–Ω–µ—Ñ—Ç', '–∞–ª—Ä–æ—Å–∞', '–ø–æ–ª—é—Å', '—Ñ–æ—Å–∞–≥—Ä–æ'
        ]
    
    def load_sources(self):
        default_sources = {
            "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å": {
                "url": "https://www.interfax.ru/rss.asp",
                "category": "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å",
                "priority": 2,
                "keywords": [],
                "enabled": True
            },
            "–†–ë–ö": {
                "url": "https://rssexport.rbc.ru/rbcnews/news/30/full.rss",
                "category": "–†–ë–ö",
                "priority": 2,
                "keywords": [],
                "enabled": True
            }
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
                return default_sources
        return default_sources
    
    def save_sources(self):
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.rss_sources, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
    
    def load_filters(self):
        default_filters = {
            'whitelist': [],
            'blacklist': []
        }
        
        if os.path.exists(self.filter_file):
            try:
                with open(self.filter_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {e}")
                return default_filters
        return default_filters
    
    def save_filters(self):
        try:
            with open(self.filter_file, 'w', encoding='utf-8') as f:
                json.dump(self.filters, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤: {e}")
    
    def calculate_priority(self, title: str, description: str, source_priority: int) -> int:
        text = f"{title} {description}".lower()
        
        for keyword in self.critical_keywords:
            if keyword in text:
                return 1
        
        for company in self.tracked_companies:
            if company in text:
                return min(source_priority, 2)
        
        return source_priority
    
    def apply_filters(self, title: str) -> bool:
        title_lower = title.lower()
        
        if self.filters['whitelist']:
            if not any(word.lower() in title_lower for word in self.filters['whitelist']):
                return False
        
        if self.filters['blacklist']:
            if any(word.lower() in title_lower for word in self.filters['blacklist']):
                return False
        
        return True
    
    async def fetch_rss_feed(self, session: aiohttp.ClientSession, source_name: str, source_config: dict) -> List[NewsItem]:
        if not source_config.get('enabled', True):
            return []
        
        urls: List[str] = []
        main_url = source_config.get('url')
        if main_url:
            urls.append(main_url)
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ alt_urls –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
        urls.extend(source_config.get('alt_urls', []))
        
        # –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ alt_urls –Ω–µ –∑–∞–¥–∞–Ω, –ø–æ–¥—Å—Ç–∞–≤–∏–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∑–µ—Ä–∫–∞–ª–∞ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if not source_config.get('alt_urls'):
            key = source_name.lower().strip()
            mirrors = MIRROR_FALLBACKS.get(key, [])
            # –Ω–µ –¥—É–±–ª–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π url
            for m in mirrors:
                if m and m not in urls:
                    urls.append(m)
        
        last_error = None
        tried_any_success = False
        for url in urls:
            for attempt in range(1, MAX_FETCH_RETRIES + 1):
                try:
                    timeout = aiohttp.ClientTimeout(total=30)
                    async with session.get(url, timeout=timeout) as response:
                        status = response.status
                        if status == 200:
                            content = await response.text()
                            feed = feedparser.parse(content)
                            news_items = []
                            # –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π URL –Ω–µ —Ä–∞–≤–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–º—É, —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –∑–µ—Ä–∫–∞–ª–æ
                            is_mirror_feed = (main_url is not None and url != main_url) or ("news.google.com" in url)
                            for entry in feed.entries[:10]:
                                try:
                                    title = entry.get('title', '')
                                    link = entry.get('link', '')
                                    description = entry.get('description', '')
                                    if not title or not link:
                                        continue
                                    if not self.apply_filters(title):
                                        continue
                                    news_hash = hashlib.md5(link.encode()).hexdigest()
                                    if news_hash in self.seen_news:
                                        continue
                                    published = entry.get('published_parsed')
                                    if published:
                                        pub_date = datetime(*published[:6])
                                        if datetime.now() - pub_date > timedelta(hours=24):
                                            continue
                                    else:
                                        pub_date = datetime.now()
                                    priority = self.calculate_priority(title, description, source_config.get('priority', 3))
                                    news_item = NewsItem(
                                        title=title,
                                        url=link,
                                        source=source_name,
                                        priority=priority,
                                        category=source_config.get('category', source_name),
                                        timestamp=pub_date,
                                        hash=news_hash,
                                        via_mirror=is_mirror_feed
                                    )
                                    self.seen_news.add(news_hash)
                                    news_items.append(news_item)
                                except Exception as e:
                                    logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ {source_name}: {e}")
                                    continue
                            tried_any_success = True
                            return news_items
                        else:
                            if status in (404, 406):
                                logging.warning(f"{source_name}: HTTP {status}. URL: {url}. –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_FETCH_RETRIES}. –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É/–ø–æ–≤—Ç–æ—Ä‚Ä¶")
                            elif status in (403, 451):
                                logging.warning(f"{source_name}: –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω (HTTP {status}). URL: {url}. –ü–æ–ø—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫‚Ä¶")
                            else:
                                logging.warning(f"{source_name}: –≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ (HTTP {status}). URL: {url}. –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_FETCH_RETRIES}‚Ä¶")
                            last_error = f"HTTP {status}"
                except Exception as e:
                    last_error = str(e)
                    logging.warning(f"{source_name}: –æ—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ URL {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_FETCH_RETRIES}): {e}")
                
                if attempt < MAX_FETCH_RETRIES:
                    backoff = BASE_BACKOFF_SEC * (2 ** (attempt - 1))
                    await asyncio.sleep(backoff)
        
        if not tried_any_success:
            tried_list = ", ".join(urls) if urls else "<–ø—É—Å—Ç–æ>"
            logging.error(f"{source_name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ª–µ–Ω—Ç—É –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}. –ü—Ä–æ–±–æ–≤–∞–Ω–Ω—ã–µ URL: {tried_list}")
        return []
    
    async def send_telegram_message(self, session: aiohttp.ClientSession, message: str):
        try:
            url = f"https://api.telegram.org/bot{self.bot_token}/sendMessage"
            data = {
                'chat_id': self.chat_id,
                'text': message,
                'parse_mode': 'HTML',
                'disable_web_page_preview': True
            }
            
            async with session.post(url, json=data) as response:
                if response.status != 200:
                    logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {await response.text()}")
                else:
                    logging.info(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
                    
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
    
    async def check_all_sources(self):
        # –°–µ—Å—Å–∏—è —Å –æ–±—â–∏–º–∏ –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
        timeout = aiohttp.ClientTimeout(total=40)
        async with aiohttp.ClientSession(headers=DEFAULT_HEADERS, timeout=timeout) as session:
            tasks = []
            for source_name, source_config in self.rss_sources.items():
                tasks.append(self.fetch_rss_feed(session, source_name, source_config))
            
            all_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            all_news = []
            for result in all_results:
                if isinstance(result, list):
                    all_news.extend(result)
            
            all_news.sort(key=lambda x: x.priority)
            
            for news in all_news:
                if not self.is_running:
                    break
                message = self.format_news_message(news)
                await self.send_telegram_message(session, message)
                
                if news.priority == 1:
                    await asyncio.sleep(0.5)
                else:
                    await asyncio.sleep(2)
    
    def format_news_message(self, news: NewsItem) -> str:
        priority_emoji = {1: 'üö®', 2: '‚ö°', 3: 'üìä', 4: 'üì∞'}
        category_emoji = {
            '–¶–ë –†–§': 'üè¶',
            '–ö—Ä–µ–º–ª—å': 'üèõÔ∏è',
            '–†–ë–ö': 'üì∫',
            '–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å': 'üì°',
            '–í–µ–¥–æ–º–æ—Å—Ç–∏': 'üì∞',
            '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç': 'üíº',
            '–§–∏–Ω–º–∞—Ä–∫–µ—Ç': 'üìà',
            '–ë–∞–Ω–∫–∏.—Ä—É': 'üèß'
        }
        
        emoji = priority_emoji.get(news.priority, 'üì∞')
        source_emoji = category_emoji.get(news.category, 'üì∞')
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è –≤ —Å–∞–º–∞—Ä—Å–∫–æ–µ
        samara_time = news.timestamp.astimezone(SAMARA_TZ)
        
        mirror_note = " \u00b7 via –∑–µ—Ä–∫–∞–ª–æ" if news.via_mirror else ""
        
        message = f"{emoji} {source_emoji} <b>{news.source}</b>{mirror_note}\n\n"
        message += f"{news.title}\n\n"
        message += f"üîó {news.url}\n"
        message += f"‚è∞ {samara_time.strftime('%H:%M:%S')}"
        
        return message
    
    async def run_monitoring(self, interval_minutes: int = 2):
        logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ñ–æ–Ω–¥–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ (–∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval_minutes} –º–∏–Ω)")
        self.is_running = True
        
        while self.is_running:
            try:
                await self.check_all_sources()
                logging.info(f"‚úÖ –¶–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω. –°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {interval_minutes} –º–∏–Ω.")
                
                for _ in range(interval_minutes * 60):
                    if not self.is_running:
                        break
                    await asyncio.sleep(1)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                await asyncio.sleep(60)
    
    def stop_monitoring(self):
        self.is_running = False
        logging.info("‚èπÔ∏è –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

async def main():
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    chat_id = os.getenv('TELEGRAM_CHAT_ID')
    interval = int(os.getenv('CHECK_INTERVAL_MINUTES', '2'))
    
    if not bot_token or not chat_id:
        logging.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω—ã TELEGRAM_BOT_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        return
    
    # –ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞—Å—ã–ø–∞–Ω–∏—è
    from aiohttp import web
    
    async def health_check(request):
        return web.Response(text="‚úÖ Bot is alive and working!")
    
    app = web.Application()
    app.router.add_get('/', health_check)
    app.router.add_get('/health', health_check)
    
    runner = web.AppRunner(app)
    await runner.setup()
    
    port = int(os.getenv('PORT', '8080'))
    site = web.TCPSite(runner, '0.0.0.0', port)
    await site.start()
    logging.info(f"üåê HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    
    # –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
    bot = RussianMarketNewsBot(bot_token, chat_id)
    
    try:
        await bot.run_monitoring(interval)
    except KeyboardInterrupt:
        logging.info("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
        bot.stop_monitoring()
    except Exception as e:
        logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    asyncio.run(main())




